# PyTorch NLP Projects Portfolio

A collection of 5 advanced PyTorch projects demonstrating expertise in Natural Language Processing and transformer architectures. Each project implements a state-of-the-art language model from scratch without relying on pre-built transformer libraries.

## Projects Overview

### 1. Custom BERT Implementation
A complete implementation of BERT (Bidirectional Encoder Representations from Transformers) built from scratch, demonstrating deep understanding of the transformer encoder architecture and self-attention mechanisms.

### 2. GPT-2 Implementation
A from-scratch implementation of the GPT-2 autoregressive language model with text generation capabilities, showcasing understanding of decoder-only transformer architectures.

### 3. Neural Machine Translation Transformer
A complete encoder-decoder transformer for sequence-to-sequence tasks like machine translation, following the original "Attention Is All You Need" architecture.

### 4. T5 (Text-to-Text Transfer Transformer)
Implementation of the T5 model architecture with relative positional encoding and the text-to-text framework that unifies NLP tasks.

### 5. RoBERTa Implementation
A RoBERTa model implementation with improvements over BERT, including dynamic masking and task-specific heads for various downstream applications.

## Technical Skills Demonstrated

- **Deep Understanding of Transformer Architectures**: Encoder-only, decoder-only, and encoder-decoder implementations
- **Advanced PyTorch Usage**: Custom layers, attention mechanisms, and efficient tensor operations
- **NLP Fundamentals**: Tokenization, embeddings, positional encoding, and language modeling
- **Model Training Techniques**: Masked language modeling, causal language modeling, and sequence-to-sequence training
- **Generation Strategies**: Beam search, top-k, and nucleus sampling for text generation

## Requirements

All projects require:
- Python 3.7+
- PyTorch 1.7.0+
- NumPy 1.19.0+

## Experience

These projects demonstrate over 4 years of experience with PyTorch and NLP, with particular expertise in transformer architectures and language model implementation.